{
 "metadata": {
  "language": "Julia",
  "name": "",
  "signature": "sha256:32122d6953462e483a59486990a8df16832d2c7470e22c4cc17ba26f9f803966"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A while ago Dahua Lin posted a message on julia-stats calling for construction of a new language (within Julia) that would take general graphical models (in abstract form) and 'compile' them to Julia code. This would free modellers of the hassles of re-implementing a whole bunch of inference algorithms (like EM) over and over again. It's a very intriguing idea and would be awesome if it could be made to work. The attached slides are here (I'm linking it because I will be referring to it in this write-up).\n",
      "\n",
      "Inspired by that, I decided to use the Distributions.jl package as a starting point and see how far I could go just by defining types and generic functions on those types to do various inference procedures (such as message passing). Note that I'm not defining a new language, just exploring what the Julia compiler itself can do. I found, pleasantly enough, that the interface Distributions.jl provides is rich enough to implement EM on both models Lin mentions in his presentation, just by some minimal EM code, specifying the high-level model and using minimal 'grunt' details. To whet your appetite, here's a generic implementation of the EM algorithm that works, without modification, on any type of mixture.\n",
      "Disclaimer: None of the code listed here is meant to be the 'optimal' way of doing things; it's just given as an example and, mostly, for fun.\n",
      "\n",
      "First, let's define a generic mixture type:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "using Distributions\n",
      "\n",
      "# A 'closed' mixture model defining a full generative model\n",
      "type MixtureModel{T} <: Distribution\n",
      "\tmixing::Distribution   # Distribution over mixing components\n",
      "\tcomponent::Vector{T}   # Individual component distributions\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, T is the type of mixture component. So, for instance, if T were MvNormal, it would give you a multivariate normal mixture.\n",
      "Now that we have that, we can do EM updates on that generic mixture type. The following function fit_mm_em does that:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This returns, for a model and observations x,\n",
      "# the distribution over latent variables.\n",
      "function infer(m::MixtureModel, x)\n",
      "\tK = length(m.component)  # number of mixture components\n",
      "\tN = size(x,2)            # number of data points\n",
      "\t\n",
      "\tlq = Array(Float64, N, K)\n",
      "\tfor k = 1:K\n",
      "\t\tlq[:,k] = logpdf(m.component[k], x) .+ logpdf(m.mixing, k)\n",
      "\tend\n",
      "\treturn lq\n",
      "end\n",
      "\n",
      "logp_to_p(lp) = exp(lp .- maximum(lp))\n",
      "\n",
      "function fit_mm_em{T}(m::MixtureModel{T}, x)\n",
      "\t# Expectation step\n",
      "\tlq = infer(m, x)\n",
      "\n",
      "\t# Normalize log-probability and convert to probability\n",
      "\tq = logp_to_p(lq)\n",
      "\tq = q ./ sum(q,2)\n",
      "\n",
      "\t# Maximization step\n",
      "\tcr = 1:length(m.component)\n",
      "\tcomps = [fit_em(m.component[k], x, q[:,k]) for k = cr]\n",
      "\tmix   =  fit_em(m.mixing, [cr], vec(sum(q,1)))\n",
      "\n",
      "\tMixtureModel{T}(mix, comps)\n",
      "end\n",
      "\n",
      "# 'fallback' function\n",
      "fit_em(m::Distribution, x, w) = fit_mle(typeof(m), x, w)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "fit_em (generic function with 1 method)"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can test this on a simple Gaussian mixture:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Data: two clusters separated by a distance of 6\n",
      "x = cat(2, randn(4,64).-3, randn(4,32).+3)\n",
      "\n",
      "# Initial guess for components and mixture weights (random)\n",
      "sigma = 0.1*cov(x')   # empirical guesstimate\n",
      "comps = [MvNormal(randn(4),sigma), MvNormal(randn(4),sigma)]\n",
      "m     = MixtureModel(Categorical([0.5, 0.5]), comps)\n",
      "\n",
      "# Run EM algorithm for a few iterations\n",
      "for i = 1:10\n",
      "\tm = fit_mm_em(m, x)\n",
      "end\n",
      "\n",
      "println(m.component[1].\u03bc)\n",
      "println(m.component[2].\u03bc)\n",
      "println(m.component[2].\u03a3)\n",
      "println(m.mixing.prob)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[-"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3.2631184236532436,-2.7006729386547077,-2.919006575979322,-2.9576009114761903]\n",
        "[1.2706768333967895,0.6933642515141178,0.6758533469320562,0.819694302494376]\n",
        "PDMat(4,[5.871224385261959 6.904157012061517 6.448055535687115 6.721782065067239\n",
        " 6.904157012061517 9.952228780963178 8.080981415827772 8.570001248647637\n",
        " 6.448055535687115 8.080981415827772 8.921371836004822 7.9947845428335755\n",
        " 6.721782065067239 8.570001248647637 7.9947845428335755 9.21604456035357],Cholesky{Float64} with factor:\n",
        "[2.42306095368275 2.8493534186864182 2.6611198227955746 2.774087071499778\n",
        " 0.0 1.3540361429382892 0.36816635494620326 0.491601920087216\n",
        " 0.0 0.0 1.3054756450485387 0.3306190822774627\n",
        " 0.0 0.0 0.0 1.0814361075403056])\n",
        "[0.47608155916462025,0.5239184408353796]\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ok, so we have a generalized EM procedure for mixtures, which compiles to a concrete function once it's called. But will it work on wacky new custom distributions? Let's find out. Let's define a new distribution type that encapsulates a distribution along with its prior. We use this to model the \u03c0 and \u03bc variables in page 9 of Lin's presentation. We also have to "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import Distributions:logpdf\n",
      "\n",
      "type DistWithPrior <: Distribution\n",
      "\tpri                    # a prior over the parameters of dist (tuple)\n",
      "\tdist::Distribution     # the distribution itself\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's also define fit_em and log_pdf on these distributions, using Distributions.jl's own fit_map function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fit_em{T<:DistWithPrior}(m::T, x, w) = \n",
      "  T(m.pri, fit_map(m.pri, typeof(m.dist), x, w))\n",
      "\n",
      "logpdf{T<:DistWithPrior}(m::T, x) = logpdf(m.dist, x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "logpdf (generic function with 48 methods)"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# dimensionality of output\n",
      "K = 4\n",
      "\n",
      "# sample data\n",
      "x = cat(2, randn(K,64).-3, randn(K,32).+3)\n",
      "\n",
      "# Diagonal prior (note that we can't use IsoNormal)\n",
      "v = 0.0\n",
      "sig = 1.0\n",
      "mu = MvNormal(fill(v,K), diagm(fill(sig,K)))\n",
      "\n",
      "# Initial guess for components and mixture weights (random)\n",
      "# note that, here, sigma is shared.\n",
      "comps = Array(DistWithPrior, 2)\n",
      "sigma = eye(4)\n",
      "comps[1] = DistWithPrior((mu, sigma), MvNormal(randn(K), sigma))\n",
      "comps[2] = DistWithPrior((mu, sigma), MvNormal(randn(K), sigma))\n",
      "\n",
      "# Dirichlet prior\n",
      "alpha = 2.0\n",
      "mix = DistWithPrior(Dirichlet(fill(alpha/2,2)), Categorical([0.5, 0.5]))\n",
      "\n",
      "m = MixtureModel(mix, comps)\n",
      "\n",
      "# Run EM algorithm for a few iterations\n",
      "for i = 1:10\n",
      "\tm = fit_mm_em(m, x)\n",
      "end\n",
      "\n",
      "println(m.component[1].dist.\u03bc)\n",
      "println(m.component[2].dist.\u03bc)\n",
      "println(m.component[2].dist.\u03a3)\n",
      "println(m.mixing.dist.prob)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[-"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2.8119260099645573,-3.083135355368115,-3.087427589202475,-2.6317364980559486]\n",
        "[3.027306305523113,2.98335879132994,2.8756961657062967,2.7799416039825484]\n",
        "PDMat"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(4,[1.0 0.0 0.0 0.0\n",
        " 0.0 1.0 0.0 0.0\n",
        " 0.0 0.0 1.0 0.0\n",
        " 0.0 0.0 0.0 1.0],Cholesky{Float64} with factor:\n",
        "[1.0 0.0 0.0 0.0\n",
        " 0.0 1.0 0.0 0.0\n",
        " 0.0 0.0 1.0 0.0\n",
        " 0.0 0.0 0.0 1.0])\n",
        "[0.6666666666666666,0.3333333333333333]\n"
       ]
      }
     ],
     "prompt_number": 6
    }
   ],
   "metadata": {}
  }
 ]
}